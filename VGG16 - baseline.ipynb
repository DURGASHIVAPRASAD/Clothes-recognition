{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "# Importing other necessary libraries\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import h5py, os, itertools, heapq\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model = keras.applications.vgg16.VGG16()\n",
    "vgg16_model.summary()  #Original VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 6006      \n",
      "=================================================================\n",
      "Total params: 134,266,550\n",
      "Trainable params: 6,006\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now freeze all top layers, delete last, add own last layer\n",
    "model = Sequential()\n",
    "for layer in vgg16_model.layers:\n",
    "    model.add(layer)\n",
    "\n",
    "# pop removed last layer (predictions (dense) 1000)\n",
    "model.layers.pop()  \n",
    "\n",
    "#freeze the top layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add my own 6 class layer\n",
    "model.add(Dense(units = 6, activation = 'softmax'))  \n",
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5831 images belonging to 6 classes.\n",
      "Found 255 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   width_shift_range = 0.1,\n",
    "                                   height_shift_range = 0.1,\n",
    "                                   rotation_range = 40,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('./dataset/training_set',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('./dataset/validation_set',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks (save weights while training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting callbacks parameters\n",
    "checkpointer = ModelCheckpoint(filepath='vgg16.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)\n",
    "filename='vgg16.csv'\n",
    "csv_log = CSVLogger(filename, separator=',', append=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "182/182 [==============================] - 47s 260ms/step - loss: 1.7828 - acc: 0.2117 - val_loss: 1.7821 - val_acc: 0.2143\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.78211, saving model to vgg16.01-1.78.hdf5\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 42s 232ms/step - loss: 1.7594 - acc: 0.2511 - val_loss: 1.7844 - val_acc: 0.1696\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.78211\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.7213 - acc: 0.2632 - val_loss: 1.7085 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.78211 to 1.70855, saving model to vgg16.03-1.71.hdf5\n",
      "Epoch 4/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.6891 - acc: 0.2779 - val_loss: 1.7046 - val_acc: 0.2277\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.70855 to 1.70464, saving model to vgg16.04-1.70.hdf5\n",
      "Epoch 5/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.6519 - acc: 0.2979 - val_loss: 1.6313 - val_acc: 0.3170\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.70464 to 1.63131, saving model to vgg16.05-1.63.hdf5\n",
      "Epoch 6/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.6424 - acc: 0.2821 - val_loss: 1.6280 - val_acc: 0.2768\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.63131 to 1.62798, saving model to vgg16.06-1.63.hdf5\n",
      "Epoch 7/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.6149 - acc: 0.2968 - val_loss: 1.6131 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.62798 to 1.61306, saving model to vgg16.07-1.61.hdf5\n",
      "Epoch 8/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.5922 - acc: 0.2984 - val_loss: 1.5865 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.61306 to 1.58655, saving model to vgg16.08-1.59.hdf5\n",
      "Epoch 9/200\n",
      "182/182 [==============================] - 42s 233ms/step - loss: 1.5744 - acc: 0.2999 - val_loss: 1.5680 - val_acc: 0.2679\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.58655 to 1.56801, saving model to vgg16.09-1.57.hdf5\n",
      "Epoch 10/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.5617 - acc: 0.3061 - val_loss: 1.5322 - val_acc: 0.2723\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.56801 to 1.53219, saving model to vgg16.10-1.53.hdf5\n",
      "Epoch 11/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.5578 - acc: 0.2991 - val_loss: 1.5132 - val_acc: 0.2723\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.53219 to 1.51315, saving model to vgg16.11-1.51.hdf5\n",
      "Epoch 12/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.5499 - acc: 0.3002 - val_loss: 1.5283 - val_acc: 0.2500\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.51315\n",
      "Epoch 13/200\n",
      "182/182 [==============================] - 42s 233ms/step - loss: 1.5361 - acc: 0.3056 - val_loss: 1.5210 - val_acc: 0.2723\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.51315\n",
      "Epoch 14/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.5196 - acc: 0.3070 - val_loss: 1.4651 - val_acc: 0.2768\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.51315 to 1.46506, saving model to vgg16.14-1.47.hdf5\n",
      "Epoch 15/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.5165 - acc: 0.3054 - val_loss: 1.4600 - val_acc: 0.2946\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.46506 to 1.45996, saving model to vgg16.15-1.46.hdf5\n",
      "Epoch 16/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.5142 - acc: 0.3059 - val_loss: 1.4507 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.45996 to 1.45067, saving model to vgg16.16-1.45.hdf5\n",
      "Epoch 17/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.5044 - acc: 0.2977 - val_loss: 1.4534 - val_acc: 0.2723\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.45067\n",
      "Epoch 18/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.4975 - acc: 0.3073 - val_loss: 1.4328 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.45067 to 1.43281, saving model to vgg16.18-1.43.hdf5\n",
      "Epoch 19/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.4931 - acc: 0.3067 - val_loss: 1.4235 - val_acc: 0.2768\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.43281 to 1.42345, saving model to vgg16.19-1.42.hdf5\n",
      "Epoch 20/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.4776 - acc: 0.3070 - val_loss: 1.3927 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.42345 to 1.39273, saving model to vgg16.20-1.39.hdf5\n",
      "Epoch 21/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.4893 - acc: 0.3067 - val_loss: 1.4168 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.39273\n",
      "Epoch 22/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.4800 - acc: 0.3089 - val_loss: 1.4225 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.39273\n",
      "Epoch 23/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.4672 - acc: 0.3096 - val_loss: 1.3642 - val_acc: 0.2768\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.39273 to 1.36418, saving model to vgg16.23-1.36.hdf5\n",
      "Epoch 24/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.4493 - acc: 0.3107 - val_loss: 1.3901 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.36418\n",
      "Epoch 25/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.4506 - acc: 0.3065 - val_loss: 1.3384 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.36418 to 1.33836, saving model to vgg16.25-1.34.hdf5\n",
      "Epoch 26/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.4324 - acc: 0.3198 - val_loss: 1.3515 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.33836\n",
      "Epoch 27/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.4328 - acc: 0.3063 - val_loss: 1.3279 - val_acc: 0.2946\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.33836 to 1.32791, saving model to vgg16.27-1.33.hdf5\n",
      "Epoch 28/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.4131 - acc: 0.3115 - val_loss: 1.4912 - val_acc: 0.2679\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.32791\n",
      "Epoch 29/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.4137 - acc: 0.2990 - val_loss: 1.3160 - val_acc: 0.2991\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.32791 to 1.31600, saving model to vgg16.29-1.32.hdf5\n",
      "Epoch 30/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.4159 - acc: 0.3096 - val_loss: 1.3744 - val_acc: 0.3036\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.31600\n",
      "Epoch 31/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.4065 - acc: 0.3061 - val_loss: 1.3971 - val_acc: 0.2768\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.31600\n",
      "Epoch 32/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.4165 - acc: 0.3036 - val_loss: 1.4205 - val_acc: 0.2500\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.31600\n",
      "Epoch 33/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.4028 - acc: 0.3044 - val_loss: 1.3855 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.31600\n",
      "Epoch 34/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.4144 - acc: 0.3032 - val_loss: 1.3093 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.31600 to 1.30926, saving model to vgg16.34-1.31.hdf5\n",
      "Epoch 35/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.4223 - acc: 0.3072 - val_loss: 1.3379 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.30926\n",
      "Epoch 36/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3861 - acc: 0.3134 - val_loss: 1.2880 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.30926 to 1.28803, saving model to vgg16.36-1.29.hdf5\n",
      "Epoch 37/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3803 - acc: 0.3084 - val_loss: 1.3028 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.28803\n",
      "Epoch 38/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3798 - acc: 0.3138 - val_loss: 1.2816 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.28803 to 1.28164, saving model to vgg16.38-1.28.hdf5\n",
      "Epoch 39/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3975 - acc: 0.2998 - val_loss: 1.3972 - val_acc: 0.2679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_loss did not improve from 1.28164\n",
      "Epoch 40/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3836 - acc: 0.2970 - val_loss: 1.2794 - val_acc: 0.2768\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.28164 to 1.27937, saving model to vgg16.40-1.28.hdf5\n",
      "Epoch 41/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3789 - acc: 0.3084 - val_loss: 1.3896 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.27937\n",
      "Epoch 42/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3745 - acc: 0.3059 - val_loss: 1.4397 - val_acc: 0.2589\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.27937\n",
      "Epoch 43/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3867 - acc: 0.3046 - val_loss: 1.2807 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.27937\n",
      "Epoch 44/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.3607 - acc: 0.3117 - val_loss: 1.3516 - val_acc: 0.2768\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.27937\n",
      "Epoch 45/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3622 - acc: 0.3045 - val_loss: 1.2853 - val_acc: 0.2723\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.27937\n",
      "Epoch 46/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3685 - acc: 0.3000 - val_loss: 1.2738 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.27937 to 1.27382, saving model to vgg16.46-1.27.hdf5\n",
      "Epoch 47/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3850 - acc: 0.2957 - val_loss: 1.2781 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.27382\n",
      "Epoch 48/200\n",
      "182/182 [==============================] - 279s 2s/step - loss: 1.3749 - acc: 0.2930 - val_loss: 1.3252 - val_acc: 0.2946\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.27382\n",
      "Epoch 49/200\n",
      "182/182 [==============================] - 386s 2s/step - loss: 1.3671 - acc: 0.3025 - val_loss: 1.3618 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.27382\n",
      "Epoch 50/200\n",
      "182/182 [==============================] - 386s 2s/step - loss: 1.3875 - acc: 0.2872 - val_loss: 1.2844 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.27382\n",
      "Epoch 51/200\n",
      "182/182 [==============================] - 385s 2s/step - loss: 1.3636 - acc: 0.3050 - val_loss: 1.2712 - val_acc: 0.2768\n",
      "\n",
      "Epoch 00051: val_loss improved from 1.27382 to 1.27123, saving model to vgg16.51-1.27.hdf5\n",
      "Epoch 52/200\n",
      "182/182 [==============================] - 388s 2s/step - loss: 1.3665 - acc: 0.2918 - val_loss: 1.2844 - val_acc: 0.3661\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.27123\n",
      "Epoch 53/200\n",
      "182/182 [==============================] - 73s 401ms/step - loss: 1.3695 - acc: 0.3064 - val_loss: 1.2625 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00053: val_loss improved from 1.27123 to 1.26254, saving model to vgg16.53-1.26.hdf5\n",
      "Epoch 54/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.3614 - acc: 0.2960 - val_loss: 1.3097 - val_acc: 0.3393\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.26254\n",
      "Epoch 55/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3581 - acc: 0.2917 - val_loss: 1.2558 - val_acc: 0.2946\n",
      "\n",
      "Epoch 00055: val_loss improved from 1.26254 to 1.25582, saving model to vgg16.55-1.26.hdf5\n",
      "Epoch 56/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.3352 - acc: 0.3095 - val_loss: 1.2642 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.25582\n",
      "Epoch 57/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.3688 - acc: 0.2937 - val_loss: 1.3245 - val_acc: 0.3036\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.25582\n",
      "Epoch 58/200\n",
      "182/182 [==============================] - 89s 487ms/step - loss: 1.3601 - acc: 0.2949 - val_loss: 1.2848 - val_acc: 0.3080\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.25582\n",
      "Epoch 59/200\n",
      "182/182 [==============================] - 295s 2s/step - loss: 1.3735 - acc: 0.2924 - val_loss: 1.2601 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.25582\n",
      "Epoch 60/200\n",
      "182/182 [==============================] - 296s 2s/step - loss: 1.3469 - acc: 0.3028 - val_loss: 1.3638 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.25582\n",
      "Epoch 61/200\n",
      "182/182 [==============================] - 296s 2s/step - loss: 1.3328 - acc: 0.2974 - val_loss: 1.2880 - val_acc: 0.2946\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.25582\n",
      "Epoch 62/200\n",
      "182/182 [==============================] - 296s 2s/step - loss: 1.3847 - acc: 0.2819 - val_loss: 1.4708 - val_acc: 0.2679\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.25582\n",
      "Epoch 63/200\n",
      "182/182 [==============================] - 299s 2s/step - loss: 1.3618 - acc: 0.2988 - val_loss: 1.3528 - val_acc: 0.3170\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.25582\n",
      "Epoch 64/200\n",
      "182/182 [==============================] - 297s 2s/step - loss: 1.3558 - acc: 0.3000 - val_loss: 1.2693 - val_acc: 0.3304\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.25582\n",
      "Epoch 65/200\n",
      "182/182 [==============================] - 68s 376ms/step - loss: 1.3558 - acc: 0.3060 - val_loss: 1.2719 - val_acc: 0.3170\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.25582\n",
      "Epoch 66/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3797 - acc: 0.2907 - val_loss: 1.3442 - val_acc: 0.3036\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.25582\n",
      "Epoch 67/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.3336 - acc: 0.3134 - val_loss: 1.3942 - val_acc: 0.2768\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.25582\n",
      "Epoch 68/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.3497 - acc: 0.3024 - val_loss: 1.2629 - val_acc: 0.3438\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.25582\n",
      "Epoch 69/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3339 - acc: 0.3008 - val_loss: 1.3149 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.25582\n",
      "Epoch 70/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.3807 - acc: 0.2988 - val_loss: 1.3159 - val_acc: 0.3080\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.25582\n",
      "Epoch 71/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3424 - acc: 0.3060 - val_loss: 1.2581 - val_acc: 0.3214\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.25582\n",
      "Epoch 72/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3368 - acc: 0.3039 - val_loss: 1.2704 - val_acc: 0.3214\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.25582\n",
      "Epoch 73/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3409 - acc: 0.3005 - val_loss: 1.2581 - val_acc: 0.3080\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.25582\n",
      "Epoch 74/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.3520 - acc: 0.3077 - val_loss: 1.2660 - val_acc: 0.3080\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.25582\n",
      "Epoch 75/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3377 - acc: 0.3084 - val_loss: 1.2996 - val_acc: 0.3348\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.25582\n",
      "Epoch 76/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3488 - acc: 0.2982 - val_loss: 1.3723 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.25582\n",
      "Epoch 77/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3537 - acc: 0.3050 - val_loss: 1.2832 - val_acc: 0.3214\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.25582\n",
      "Epoch 78/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3460 - acc: 0.2976 - val_loss: 1.3060 - val_acc: 0.3304\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.25582\n",
      "Epoch 79/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3411 - acc: 0.3041 - val_loss: 1.2647 - val_acc: 0.3438\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.25582\n",
      "Epoch 80/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3444 - acc: 0.3057 - val_loss: 1.2531 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00080: val_loss improved from 1.25582 to 1.25312, saving model to vgg16.80-1.25.hdf5\n",
      "Epoch 81/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3376 - acc: 0.3137 - val_loss: 1.2540 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.25312\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3331 - acc: 0.3049 - val_loss: 1.2560 - val_acc: 0.2991\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.25312\n",
      "Epoch 83/200\n",
      "182/182 [==============================] - 43s 234ms/step - loss: 1.3395 - acc: 0.3096 - val_loss: 1.3035 - val_acc: 0.2946\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.25312\n",
      "Epoch 84/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3327 - acc: 0.3111 - val_loss: 1.3040 - val_acc: 0.2946\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.25312\n",
      "Epoch 85/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3234 - acc: 0.3061 - val_loss: 1.2507 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00085: val_loss improved from 1.25312 to 1.25070, saving model to vgg16.85-1.25.hdf5\n",
      "Epoch 86/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3428 - acc: 0.3108 - val_loss: 1.2643 - val_acc: 0.3571\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.25070\n",
      "Epoch 87/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3404 - acc: 0.3189 - val_loss: 1.3710 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.25070\n",
      "Epoch 88/200\n",
      "182/182 [==============================] - 43s 238ms/step - loss: 1.3388 - acc: 0.3049 - val_loss: 1.4167 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.25070\n",
      "Epoch 89/200\n",
      "182/182 [==============================] - 43s 238ms/step - loss: 1.3658 - acc: 0.3060 - val_loss: 1.3383 - val_acc: 0.3036\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.25070\n",
      "Epoch 90/200\n",
      "182/182 [==============================] - 44s 239ms/step - loss: 1.3525 - acc: 0.3049 - val_loss: 1.3497 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.25070\n",
      "Epoch 91/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3310 - acc: 0.3081 - val_loss: 1.2729 - val_acc: 0.3304\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.25070\n",
      "Epoch 92/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.3362 - acc: 0.3051 - val_loss: 1.2475 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00092: val_loss improved from 1.25070 to 1.24753, saving model to vgg16.92-1.25.hdf5\n",
      "Epoch 93/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3382 - acc: 0.3082 - val_loss: 1.3972 - val_acc: 0.3036\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.24753\n",
      "Epoch 94/200\n",
      "182/182 [==============================] - 43s 238ms/step - loss: 1.3387 - acc: 0.3025 - val_loss: 1.4153 - val_acc: 0.3036\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.24753\n",
      "Epoch 95/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.3411 - acc: 0.3093 - val_loss: 1.2442 - val_acc: 0.3036\n",
      "\n",
      "Epoch 00095: val_loss improved from 1.24753 to 1.24421, saving model to vgg16.95-1.24.hdf5\n",
      "Epoch 96/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.3310 - acc: 0.3168 - val_loss: 1.3242 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.24421\n",
      "Epoch 97/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3280 - acc: 0.3159 - val_loss: 1.2714 - val_acc: 0.3036\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.24421\n",
      "Epoch 98/200\n",
      "182/182 [==============================] - 43s 238ms/step - loss: 1.3180 - acc: 0.3171 - val_loss: 1.2419 - val_acc: 0.3259\n",
      "\n",
      "Epoch 00098: val_loss improved from 1.24421 to 1.24193, saving model to vgg16.98-1.24.hdf5\n",
      "Epoch 99/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.3506 - acc: 0.3166 - val_loss: 1.2845 - val_acc: 0.3036\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.24193\n",
      "Epoch 100/200\n",
      "182/182 [==============================] - 44s 240ms/step - loss: 1.3304 - acc: 0.3112 - val_loss: 1.2439 - val_acc: 0.3482\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.24193\n",
      "Epoch 101/200\n",
      "182/182 [==============================] - 44s 241ms/step - loss: 1.3341 - acc: 0.3114 - val_loss: 1.3425 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.24193\n",
      "Epoch 102/200\n",
      "182/182 [==============================] - 44s 241ms/step - loss: 1.3272 - acc: 0.3182 - val_loss: 1.2557 - val_acc: 0.2991\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.24193\n",
      "Epoch 103/200\n",
      "182/182 [==============================] - 44s 240ms/step - loss: 1.3369 - acc: 0.3099 - val_loss: 1.2531 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.24193\n",
      "Epoch 104/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3296 - acc: 0.3055 - val_loss: 1.3731 - val_acc: 0.2991\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.24193\n",
      "Epoch 105/200\n",
      "182/182 [==============================] - 43s 237ms/step - loss: 1.3247 - acc: 0.3113 - val_loss: 1.2576 - val_acc: 0.3438\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.24193\n",
      "Epoch 106/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3219 - acc: 0.3202 - val_loss: 1.2429 - val_acc: 0.3080\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.24193\n",
      "Epoch 107/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3284 - acc: 0.3135 - val_loss: 1.2939 - val_acc: 0.2991\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.24193\n",
      "Epoch 108/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3192 - acc: 0.3177 - val_loss: 1.2850 - val_acc: 0.3393\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.24193\n",
      "Epoch 109/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3274 - acc: 0.3155 - val_loss: 1.3555 - val_acc: 0.2991\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.24193\n",
      "Epoch 110/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3275 - acc: 0.3105 - val_loss: 1.2476 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.24193\n",
      "Epoch 111/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3300 - acc: 0.3178 - val_loss: 1.2341 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00111: val_loss improved from 1.24193 to 1.23407, saving model to vgg16.111-1.23.hdf5\n",
      "Epoch 112/200\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 1.3421 - acc: 0.3089 - val_loss: 1.2365 - val_acc: 0.3304\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.23407\n",
      "Epoch 113/200\n",
      "182/182 [==============================] - 43s 236ms/step - loss: 1.3257 - acc: 0.3171 - val_loss: 1.2433 - val_acc: 0.3304\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.23407\n",
      "Epoch 114/200\n",
      "117/182 [==================>...........] - ETA: 14s - loss: 1.3555 - acc: 0.3072"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cf04fb022aa1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                            \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                            \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                            callbacks = [csv_log, checkpointer])\n\u001b[0m",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2230\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2232\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\default.laptop-2ci68m4p\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "hist = model.fit_generator(training_set,\n",
    "                           steps_per_epoch = (5831//32),\n",
    "                           epochs = 200,\n",
    "                           validation_data = test_set,\n",
    "                           validation_steps = (255//32), \n",
    "                           workers = 4, \n",
    "                           callbacks = [csv_log, checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I had to terminate this training half way, due to extremely low accuracy. No point continuing. Had to look for other solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
